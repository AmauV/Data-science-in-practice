{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections as cl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load original dataset\n",
    "churn_original = pd.read_excel(\"Churn_original.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_original = churn_original[churn_original[\"TotalCharges\"] != \" \"]\n",
    "churn_original[\"TotalCharges\"] = churn_original[\"TotalCharges\"].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load dataset with categorical columns\n",
    "churn = pd.read_excel(\"Churn.xlsx\")\n",
    "churn = churn[churn[\"TotalCharges\"] != \" \"]\n",
    "churn[\"TotalCharges\"] = churn[\"TotalCharges\"].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmaTotalCharges = churn.TotalCharges.std()\n",
    "sigmaMonthlyCharges = churn.TotalCharges.std()\n",
    "sigmaTenure = churn.tenure.std()\n",
    "meanTotalCharges = churn.TotalCharges.mean()\n",
    "meanMonthlyCharges = churn.MonthlyCharges.mean()\n",
    "meanTenure = churn.tenure.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn.TotalCharges = churn.TotalCharges.transform(lambda x: (x - meanTotalCharges) / sigmaTotalCharges)\n",
    "churn.MonthlyCharges = churn.MonthlyCharges.transform(lambda x: (x - meanMonthlyCharges) / sigmaMonthlyCharges)\n",
    "churn.tenure = churn.tenure.transform(lambda x: (x - meanTenure) / meanTenure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\"tenure\", \"MonthlyCharges\", \"TotalCharges\"]\n",
    "fig, ax = plt.subplots(1, 3, figsize=(15,5))\n",
    "for i in range (len(names)):\n",
    "    column_name = names[i]\n",
    "    ax[i].set_title(column_name)\n",
    "    ax[i].boxplot(np.array(churn_original[column_name]), 1, showfliers=True)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_corr = churn.corr()\n",
    "\n",
    "corr = []\n",
    "churn_tmp = churn.copy()\n",
    "churn_tmp.drop(\"customerID\", axis =1, inplace =  True)\n",
    "i, j = 0, 0\n",
    "\n",
    "for c1 in churn.drop(\"customerID\", axis=1).columns:\n",
    "    churn_tmp.drop(c1, axis =1, inplace =  True)\n",
    "    for c2 in churn_tmp.columns:\n",
    "        corr.append([round(churn_corr[c1].loc[c2],4),c1,c2])\n",
    "        j = j+1\n",
    "    i = i+1\n",
    "\n",
    "corr_sorted = sorted(corr, reverse=True)\n",
    "corr_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*We observe that all the columns \"No service\" contain redundant information. Indeed, the columns with values \"no Internet values\" are equal to the column \"Internet service\". That's the same for the columns \"Multiple lines\" and \"Phone service\".\n",
    "Thus, we can delete the columns 'TechSupportNoService', 'StreamingTVNoService', 'StreamingMoviesNoService', 'OnlineSecurityNoService', 'OnlineBackupNoService', 'DeviceProtectionnoService' and 'MultipleLinesNoService'*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deletion of redundant columns\n",
    "churn.drop(['TechSupportNoService', 'StreamingTVNoService', 'StreamingMoviesNoService', 'OnlineSecurityNoService', 'OnlineBackupNoService', 'DeviceProtectionnoService', 'MultipleLinesNoService'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_corr = churn.corr()\n",
    "\n",
    "corr = []\n",
    "churn_tmp = churn.copy()\n",
    "churn_tmp.drop(\"customerID\", axis =1, inplace =  True)\n",
    "i, j = 0, 0\n",
    "\n",
    "for c1 in churn.drop(\"customerID\", axis=1).columns:\n",
    "    churn_tmp.drop(c1, axis =1, inplace =  True)\n",
    "    for c2 in churn_tmp.columns:\n",
    "        corr.append([round(churn_corr[c1].loc[c2],4),c1,c2])\n",
    "        j = j+1\n",
    "    i = i+1\n",
    "\n",
    "corr_sorted = sorted(corr, reverse=True)\n",
    "corr_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Note about charges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(churn.tenure.multiply(churn.MonthlyCharges).corr(churn.TotalCharges))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*We observe that the column \"Total Charges\" is almost equal to the multiplication of the columns \"Monthly Charges\" and \"Tenure\". So we have also redundant information with those 3 columns.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observation of the values in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for column_name in churn_original.drop(\"tenure\", axis=1).columns[1:-3]:\n",
    "    print(churn_original[str(column_name)].value_counts())\n",
    "    print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "    print('\\r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First analysis of the importance of features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of categorical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we display in histograms the proportion of clients who churned for each feature. That will give us a first estimation of the importance of each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Proportion of churns for each feature: (the light gray aera is the proportion of people who didn't churn)\")\n",
    "\n",
    "columns = churn_original.drop(\"tenure\", axis=1).columns[1:-3]\n",
    "churned = churn_original[churn_original.Churn == \"Yes\"]\n",
    "\n",
    "color = [\"grey\", \"blue\", \"orange\", \"purple\", \"pink\"]\n",
    "\n",
    "fig, ax = plt.subplots(6, 3, figsize=(20,25))\n",
    "\n",
    "for i in range(len(columns)):\n",
    "    column_name = str(columns[i])\n",
    "    \n",
    "    weights = np.ones(churned.shape[0])\n",
    "    weights1 = np.ones(churned.shape[0])\n",
    "\n",
    "    for value in churned[column_name].value_counts().index.values.tolist():\n",
    "        weights[churned[column_name] == value] = 1/churn_original[column_name].value_counts().loc[value]\n",
    "        weights1[churned[column_name] == value] = 1/churned[column_name].value_counts().loc[value]\n",
    "        \n",
    "    ax[int(i/3)][i%3].hist(churned[column_name], color=\"#f2f2f2\", weights=weights1)    \n",
    "    ax[int(i/3)][i%3].hist(churned[column_name], color=color[i%5], weights=weights)\n",
    "                          \n",
    "    ax[int(i/3)][i%3].set_title(column_name)\n",
    "    ax[int(i/3)][i%3].set_xlabel(\"Values of the feature\")\n",
    "    ax[int(i/3)][i%3].set_ylabel(\"Proportion of churns\")\n",
    "    \n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*We observe a lot of things with this first analysis.<br>\n",
    "First, some features to have very little influence on the churn, such as the gender, the multiple lines, the phone service.\n",
    "Other features seem to be very important, like the tech support, the online security, the contract.<br><br>\n",
    "3 payment methods look similar but the electronic check however leads to much more churns than the others.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of the tenure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tenure = churn_original[[\"tenure\",\"Churn\"]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tenure1 = tenure[tenure.tenure <= 6].Churn\n",
    "tenure2 = tenure[np.logical_and(tenure.tenure > 6, tenure.tenure <= 20)].Churn\n",
    "tenure3 = tenure[np.logical_and(tenure.tenure > 20, tenure.tenure <= 40)].Churn\n",
    "tenure4 = tenure[np.logical_and(tenure.tenure > 40, tenure.tenure <= 60)].Churn\n",
    "tenure5 = tenure[tenure.tenure > 60].Churn\n",
    "\n",
    "tenures = [tenure1.values, tenure2.values, tenure3.values, tenure4.values, tenure5.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tenure['tenure6'] = pd.Series(tenure.tenure <= 6, tenure.index)\n",
    "tenure['tenure6_20'] = pd.Series(np.logical_and(tenure.tenure > 6, tenure.tenure <= 20), tenure.index)\n",
    "tenure['tenure20_40'] = pd.Series(np.logical_and(tenure.tenure > 20, tenure.tenure <= 40), tenure.index)\n",
    "tenure['tenure40_60'] = pd.Series(np.logical_and(tenure.tenure > 40, tenure.tenure <= 60), tenure.index)\n",
    "tenure['tenure60'] = pd.Series(tenure.tenure > 60, tenure.index)\n",
    "tenure['tenure_step'] = tenure['tenure6'] + 2 * tenure['tenure6_20'] + 3 * tenure['tenure20_40'] + 4 * tenure['tenure40_60'] + 5 * tenure['tenure60']\n",
    "tenure_step_churn = np.array(tenure[tenure.Churn == 'Yes'].tenure_step)\n",
    "tenure_step_not_churn = np.array(tenure[tenure.Churn == 'No'].tenure_step)\n",
    "churn_graph = [tenure_step_churn, tenure_step_not_churn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,5))\n",
    "ax = plt.axes()\n",
    "\n",
    "colors = ['blue', 'orange']\n",
    "\n",
    "ax.hist(churn_graph, 5, histtype='bar', label=[\"churn\", \"no churn\"],  color=colors)\n",
    "ax.legend()\n",
    "ax.set_xticks([1.4,2.2,3,3.8,4.6])\n",
    "ax.set_xticklabels([\"Tenure <= 6\",\"6 < tenure <= 20\",\"20 < Tenure <= 40\",\"40 < tenure <= 60\",\"Tenure > 60\"])\n",
    "ax.set_ylabel(\"Number of clients\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*We observe that people who are clients since a long time are less likely to churn.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, further analysis..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = churn.iloc[0:6000].drop(\"customerID\", axis=1)\n",
    "test =  churn.iloc[6001:7000].drop(\"customerID\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_score(x):\n",
    "    return np.array(x).T[0].sum() / len(x)\n",
    "\n",
    "churn.sample(frac=1)\n",
    "n = 8\n",
    "chunk_size = int(len(churn) / n)\n",
    "random_forest = []\n",
    "random_forest_proba = [] \n",
    "log_reg = []\n",
    "for i in range(n):\n",
    "    test_low = i * chunk_size\n",
    "    test_high = (i + 1) * chunk_size\n",
    "    train_index = list(range(len(churn)))\n",
    "    for j in range(test_low, test_high):\n",
    "        train_index.remove(j)\n",
    "    train = churn.iloc[train_index].drop(\"customerID\", axis=1)\n",
    "    test =  churn.iloc[test_low:test_high].drop(\"customerID\", axis=1)\n",
    "    random_forest.append(predict_random_forest(train, test))\n",
    "    random_forest_proba.append(predict_random_forest_proba(train, test))\n",
    "    log_reg.append(predict_log_reg(train, test))\n",
    "(mean_score(random_forest), mean_score(random_forest_proba), mean_score(log_reg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(score, predictor) = log_reg[0]\n",
    "predictor(test.drop(\"Churn\", axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_random_forest(train, test):\n",
    "    classifier = RandomForestClassifier()\n",
    "    classifier.fit(train.drop(\"Churn\", axis=1), train.Churn)\n",
    "    predicted_churn = classifier.predict(test.drop(\"Churn\", axis=1))\n",
    "    classifier_score = classifier.score(test.drop(\"Churn\", axis=1), test.Churn)\n",
    "    return (classifier_score, classifier.predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest_proba_classifier(x, classifier, split_value):\n",
    "    prediction = classifier.predict_proba(x)\n",
    "    prediction[prediction <= split_value] = 0\n",
    "    prediction[prediction > split_value] = 1\n",
    "    return prediction\n",
    "\n",
    "def predict_random_forest_proba(train, test):\n",
    "    classifier = RandomForestClassifier()\n",
    "    classifier.fit(train.drop(\"Churn\", axis=1), train.Churn)\n",
    "    predicted_churn = classifier.predict_proba(test.drop(\"Churn\", axis=1))\n",
    "    prediction = predicted_churn.T[1].T\n",
    "    c = cl.Counter(prediction)\n",
    "    a = sorted(list(c.items()))\n",
    "    keys = np.array(a).T[0].T\n",
    "    values = np.array(a).T[1].T\n",
    "    proportion = churn.Churn.value_counts()\n",
    "\n",
    "    counter = 0\n",
    "    index=0\n",
    "    while counter < (proportion[1] * sum(values) / (proportion [0] + proportion[1])):\n",
    "        index += 1\n",
    "        counter += values[index]\n",
    "    prediction[prediction <= keys[index]] = 0\n",
    "    prediction[prediction > keys[index]] = 1\n",
    "    matched = (prediction - np.array(test.Churn) == 0)\n",
    "    score = (prediction - np.array(test.Churn) == 0).sum()/len(test)\n",
    "    return (score, partial(random_forest_proba_classifier, classifier=classifier, split_value=keys[index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_log_reg(train, test):\n",
    "    X_train = train.drop(\"Churn\", axis=1)\n",
    "    X_test = test.drop(\"Churn\", axis=1)\n",
    "    y_train = train.Churn\n",
    "    y_test = test.Churn\n",
    "    logisticRegr = LogisticRegression()\n",
    "    logisticRegr.fit(X_train,y_train)\n",
    "    pred = logisticRegr.predict(X_test)\n",
    "    score = accuracy_score(y_test, pred)\n",
    "    return (score, logisticRegr.predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = churn.drop(['Churn','customerID'],axis = 1)\n",
    "y = churn['Churn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logisticRegr = LogisticRegression()\n",
    "logisticRegr.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = logisticRegr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_proba = logisticRegr.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
